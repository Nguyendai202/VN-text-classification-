{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyendai202/VN-text-classification-/blob/main/VN_text_classification_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Thư Viện**"
      ],
      "metadata": {
        "id": "icojPttaTdZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi \n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Xu86PZtATn9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd0c084-5f9f-4609-86a0-79855b45dc39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 40.4 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40 kB 45.2 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51 kB 48.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 52.9 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71 kB 56.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81 kB 57.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 58.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 153 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 163 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 174 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 184 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 194 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 204 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 215 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 225 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 235 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 245 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 256 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 266 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 276 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 286 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 296 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 307 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 317 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 327 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 337 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 348 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 358 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 368 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 378 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 389 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 399 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 409 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 419 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 430 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 440 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 450 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 460 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 471 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 481 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 491 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 501 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 512 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 522 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 532 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 542 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 552 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 563 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 573 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 583 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 593 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 604 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 614 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 624 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 634 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 645 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 655 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 665 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 675 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 686 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 696 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 706 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 716 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 727 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 737 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 747 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 757 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 768 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 778 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 788 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 798 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 808 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 819 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 829 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 839 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 849 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 860 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 870 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 880 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 890 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 901 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 911 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 921 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 931 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 942 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 952 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 962 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 972 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 983 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 993 kB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0 MB 60.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0 MB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->pyvi) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->pyvi) (0.8.10)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import smart_open\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.metrics import sparse_categorical_accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import os \n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "pYx3wvtMzAHp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**(tiền xử lí dữ liệu như loại bỏ kí tự đặc biệt , từ không cần thiết , chữ hoa , tách từ ..)"
      ],
      "metadata": {
        "id": "vGS5eRmqT-SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
        "dir_path = os.path.join(dir_path, 'Data')\n",
        "# '/Users/macos/Desktop/Github/NLP/Text Classifier'\n",
        "# Load data from dataset folder\n",
        "# VNTC-master/Data/10Topics/Ver1.1/Train_Full\n",
        "# VNTC-master/Data/10Topics/Ver1.1/Test_Full\n",
        "def get_data(folder_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    dirs = os.listdir(folder_path)\n",
        "    for path in tqdm(dirs):\n",
        "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
        "        for file_path in tqdm(file_paths):\n",
        "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
        "                lines = f.readlines()\n",
        "                lines = ' '.join(lines)\n",
        "                lines = gensim.utils.simple_preprocess(lines)# xoá các kí tự đặc biệt như chấm , phẩy đóng mở ngoặc \n",
        "                lines = ' '.join(lines)\n",
        "                # lines = stopwords.words(lines)\n",
        "                lines = ViTokenizer.tokenize(lines)\n",
        "\n",
        "#                 sentence = ' '.join(words)\n",
        "#                 print(lines)\n",
        "                X.append(lines)\n",
        "                y.append(path)\n",
        "#             break\n",
        "#         break\n",
        "    return X, y\n",
        "\n",
        "train_path = os.path.join(dir_path, '/content/drive/MyDrive/NLP/text-classfication/data-test/Test_Full')\n",
        "X_train, y_train = get_data(train_path)\n"
      ],
      "metadata": {
        "id": "PCY9cFZmzLnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lưu lại và tải về sau dùng , đỡ phải xử lí lại, X là văn bản đã xử lí , y là nhãn \n",
        "import pickle\n",
        "pickle.dump(X_train, open('X_test.pkl', 'wb'))\n",
        "pickle.dump(y_train, open('y_test.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "Sq6hGh5p09Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**( mã hoá các dữ liệu ta vừa tiền xử lí xong>> vector dạng số >> để đưa vào model train) "
      ],
      "metadata": {
        "id": "9LlvkN7IUOXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dữ liệu \n",
        "import pickle  # load dữ liệu \n",
        "\n",
        "X_train = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/train/X_train.pkl', 'rb'))\n",
        "y_train= pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/train/y_train.pkl', 'rb'))\n",
        "\n",
        "X_test = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/test/X_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/test/y_test.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "oNdACeRa17Eu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF ( PP thống kê này nhằm phản ánh độ quan trọng của mỗi từ hay n-gram(chuỗi từ trong n kích thước )  từ trên toàn bộ tài liêụ đầu vào , loại pp này có nhược điểm là ko biểu diễn đc nghĩa của từ ) "
      ],
      "metadata": {
        "id": "RgWzGM-m2eZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(data):\n",
        "  tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=70000, ngram_range=(1, 2))\n",
        "  tfidf_vect_ngram.fit(data)\n",
        "  X_data_tfidf_ngram =  tfidf_vect_ngram.transform(data)\n",
        "  return X_data_tfidf_ngram"
      ],
      "metadata": {
        "id": "m-ICBYgE3ZT7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nQA03SNpBOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tfidf = tfidf(X_train)\n",
        "x_test_tfidf = tfidf(X_test)\n"
      ],
      "metadata": {
        "id": "4iGKaj9jwBkn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = TruncatedSVD(n_components=300, random_state=42,)# giảm chiều dữ liệu \n",
        "X_data_tfidf_ngram_dimen =svd.fit(x_train_tfidf).transform(x_train_tfidf)\n",
        "X_test_tfidf_ngram_dimen= svd.fit(x_test_tfidf).transform(x_test_tfidf)"
      ],
      "metadata": {
        "id": "yyT7hLgcpwfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tfidf.get_shape)"
      ],
      "metadata": {
        "id": "vmCqM__gUwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec"
      ],
      "metadata": {
        "id": "L9CVB7p5v1xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os \n",
        "word2vec_model_path = '/content/drive/MyDrive/NLP/text-classfication/wordvectors-master/baomoi.vn.model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_model_path,binary=True,unicode_errors='igore')\n",
        "vocab = model.wv.vocab\n",
        "wv= model.wv\n"
      ],
      "metadata": {
        "id": "Ke-XawBSv4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mã hoá \n",
        "def get_word2vec_data(X):\n",
        "    word2vec_data = []\n",
        "    for x in X:\n",
        "        sentence = []\n",
        "        for word in x.split(\" \"):\n",
        "            if word in vocab:\n",
        "                print(word)\n",
        "                sentence.append(wv[word])\n",
        "\n",
        "        word2vec_data.append(sentence)\n",
        "#         break\n",
        "    return get_word2vec_data\n",
        "\n",
        "X_data_w2v = get_word2vec_data(X_train)\n",
        "X_test_w2v = get_word2vec_data(X_test)"
      ],
      "metadata": {
        "id": "Aeww2cn0cDoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words"
      ],
      "metadata": {
        "id": "r7Od-hMaVFQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bow(data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  databow = tokenizer.texts_to_sequences(data)\n",
        "  databow= pad_sequences(databow, maxlen=300)\n",
        "  return databow"
      ],
      "metadata": {
        "id": "BTazspdtvulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "databow_train = bow(X_train)\n",
        "databow_test = bow(X_test)\n",
        "svd = TruncatedSVD(n_components=300, random_state=42,)\n",
        "X_data_bow_dimen =svd.fit(x_train_tfidf).transform(x_train_tfidf)\n",
        "X_test_bow_dimen= svd.fit(x_test_tfidf).transform(x_test_tfidf)"
      ],
      "metadata": {
        "id": "UJCPySbLw_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CÁC PHƯƠNG PHÁP HỌC MÁY DÙNG ĐỂ PHÂN LOẠI ***"
      ],
      "metadata": {
        "id": "0glTMmo4hTeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert y to categorical"
      ],
      "metadata": {
        "id": "yZkIsprQWKj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train_n = encoder.fit_transform(y_train)\n",
        "y_test_n = encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "gF3enAT6WCaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iM7TRbeWOVO",
        "outputId": "249830a0-0dfd-4654-b088-a4d8cd7b7611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Chinh tri Xa hoi', 'Doi song', 'Kinh doanh'], dtype='<U16')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm train model "
      ],
      "metadata": {
        "id": "s6dncLK-W8Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sHgj4QeIXATN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=5000):       \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
        "    \n",
        "    if is_neuralnet:\n",
        "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
        "        \n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        val_predictions = val_predictions.argmax(axis=-1)\n",
        "        test_predictions = test_predictions.argmax(axis=-1)\n",
        "    else:\n",
        "        classifier.fit(X_train, y_train)\n",
        "    \n",
        "        train_predictions = classifier.predict(X_train)\n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        \n",
        "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
        "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n",
        "    print(classification_report(test_predictions, y_test))"
      ],
      "metadata": {
        "id": "12dsSY4VXC9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "I9s0tKnGXaL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data_tfidf_ngram_dimen, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9V0bRRrsXcZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=svm.SVC(kernel='linear',C=100)\n",
        "clf.fit(X_train, y_train)\n",
        "val_predictions = clf.predict(X_test_tfidf_ngram_dimen)\n",
        "print(classification_report(val_predictions, y_test)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J35uEVKdZ_8R",
        "outputId": "75bdc41e-cbb3-4292-ee62-b4f3efc93318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Chinh tri Xa hoi       0.60      0.58      0.59      7850\n",
            "        Doi song       0.84      0.46      0.60      3715\n",
            "      Kinh doanh       0.30      0.48      0.37      3316\n",
            "\n",
            "        accuracy                           0.53     14881\n",
            "       macro avg       0.58      0.51      0.52     14881\n",
            "    weighted avg       0.59      0.53      0.54     14881\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAIVE BAYES"
      ],
      "metadata": {
        "id": "bn1OonykaEm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(naive_bayes.MultinomialNB(),x_train_tfidf ,y_train_n,x_test_tfidf,y_test_n,is_neuralnet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjyzB0ZU-lb_",
        "outputId": "e3cb6502-7993-4eea-dc8e-809ba5e54c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.9095890410958904\n",
            "Test accuracy:  0.4420401854714065\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.66      0.46      4027\n",
            "           1       0.60      0.28      0.38      4398\n",
            "           2       0.51      0.42      0.46      6456\n",
            "\n",
            "    accuracy                           0.44     14881\n",
            "   macro avg       0.49      0.45      0.43     14881\n",
            "weighted avg       0.49      0.44      0.44     14881\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting model "
      ],
      "metadata": {
        "id": "zM_UBph_t_XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(xgboost.XGBClassifier(), X_data_tfidf_ngram_dimen, y_train_n, X_test_tfidf_ngram_dimen, y_test_n, is_neuralnet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_ceCxnluB9Y",
        "outputId": "331f81a9-3129-449e-8657-cad57e9a2960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.915068493150685\n",
            "Test accuracy:  0.5566158188293797\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.60      7735\n",
            "           1       0.96      0.39      0.55      5027\n",
            "           2       0.33      0.81      0.47      2119\n",
            "\n",
            "    accuracy                           0.56     14881\n",
            "   macro avg       0.63      0.60      0.54     14881\n",
            "weighted avg       0.69      0.56      0.57     14881\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging model "
      ],
      "metadata": {
        "id": "OlCzyQl0uh1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(xgboost.XGBClassifier(), X_data_tfidf_ngram_dimen, y_train_n, X_test_tfidf_ngram_dimen, y_test_n, is_neuralnet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gaAm6g4ufU8",
        "outputId": "0af830fc-0edd-4dea-8339-ba9cebdc8da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.915068493150685\n",
            "Test accuracy:  0.5566158188293797\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.60      0.60      7735\n",
            "           1       0.96      0.39      0.55      5027\n",
            "           2       0.33      0.81      0.47      2119\n",
            "\n",
            "    accuracy                           0.56     14881\n",
            "   macro avg       0.63      0.60      0.54     14881\n",
            "weighted avg       0.69      0.56      0.57     14881\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1fi_hbNUPdfP9_Uh4sHsgieisJMcb8IYR",
      "authorship_tag": "ABX9TyNDkGlU/0+fydtvamS0TDoG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}