{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyendai202/VN-text-classification-/blob/main/VN_text_classification_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Thư Viện**"
      ],
      "metadata": {
        "id": "icojPttaTdZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi \n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Xu86PZtATn9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import smart_open\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from keras.metrics import sparse_categorical_accuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import os \n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "pYx3wvtMzAHp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**(tiền xử lí dữ liệu như loại bỏ kí tự đặc biệt , từ không cần thiết , chữ hoa , tách từ ..)"
      ],
      "metadata": {
        "id": "vGS5eRmqT-SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
        "dir_path = os.path.join(dir_path, 'Data')\n",
        "# '/Users/macos/Desktop/Github/NLP/Text Classifier'\n",
        "# Load data from dataset folder\n",
        "# VNTC-master/Data/10Topics/Ver1.1/Train_Full\n",
        "# VNTC-master/Data/10Topics/Ver1.1/Test_Full\n",
        "def get_data(folder_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    dirs = os.listdir(folder_path)\n",
        "    for path in tqdm(dirs):\n",
        "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
        "        for file_path in tqdm(file_paths):\n",
        "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
        "                lines = f.readlines()\n",
        "                lines = ' '.join(lines)\n",
        "                lines = gensim.utils.simple_preprocess(lines)# xoá các kí tự đặc biệt như chấm , phẩy đóng mở ngoặc \n",
        "                lines = ' '.join(lines)\n",
        "                # lines = stopwords.words(lines)\n",
        "                lines = ViTokenizer.tokenize(lines)\n",
        "\n",
        "#                 sentence = ' '.join(words)\n",
        "#                 print(lines)\n",
        "                X.append(lines)\n",
        "                y.append(path)\n",
        "#             break\n",
        "#         break\n",
        "    return X, y\n",
        "\n",
        "train_path = os.path.join(dir_path, '/content/drive/MyDrive/NLP/text-classfication/data-test/Test_Full')\n",
        "X_train, y_train = get_data(train_path)\n"
      ],
      "metadata": {
        "id": "PCY9cFZmzLnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lưu lại và tải về sau dùng , đỡ phải xử lí lại, X là văn bản đã xử lí , y là nhãn \n",
        "import pickle\n",
        "pickle.dump(X_train, open('X_test.pkl', 'wb'))\n",
        "pickle.dump(y_train, open('y_test.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "Sq6hGh5p09Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering**( mã hoá các dữ liệu ta vừa tiền xử lí xong>> vector dạng số >> để đưa vào model train) "
      ],
      "metadata": {
        "id": "9LlvkN7IUOXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dữ liệu \n",
        "import pickle  # load dữ liệu \n",
        "\n",
        "X_train = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/train/X_train.pkl', 'rb'))\n",
        "y_train= pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/train/y_train.pkl', 'rb'))\n",
        "\n",
        "X_test = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/test/X_test.pkl', 'rb'))\n",
        "y_test = pickle.load(open('/content/drive/MyDrive/NLP/text-classfication/pickle-data/test/y_test.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "oNdACeRa17Eu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF ( PP thống kê này nhằm phản ánh độ quan trọng của mỗi từ hay n-gram(chuỗi từ trong n kích thước )  từ trên toàn bộ tài liêụ đầu vào , loại pp này có nhược điểm là ko biểu diễn đc nghĩa của từ ) "
      ],
      "metadata": {
        "id": "RgWzGM-m2eZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf(data):\n",
        "  tfidf_vect_ngram = TfidfVectorizer(analyzer='word', max_features=30000, ngram_range=(1, 2))\n",
        "  tfidf_vect_ngram.fit(data)\n",
        "  X_data_tfidf_ngram =  tfidf_vect_ngram.transform(data)\n",
        "  return X_data_tfidf_ngram"
      ],
      "metadata": {
        "id": "m-ICBYgE3ZT7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tfidf = tfidf(X_train)\n",
        "x_test_tfidf = tfidf(X_test)\n",
        "svd = TruncatedSVD(n_components=300, random_state=42,)# giảm chiều dữ liệu \n",
        "X_data_tfidf_ngram_dimen =svd.fit(x_train_tfidf).transform(x_train_tfidf)\n",
        "X_test_tfidf_ngram_dimen= svd.fit(x_test_tfidf).transform(x_test_tfidf)"
      ],
      "metadata": {
        "id": "4iGKaj9jwBkn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_tfidf.get_shape)"
      ],
      "metadata": {
        "id": "vmCqM__gUwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec"
      ],
      "metadata": {
        "id": "L9CVB7p5v1xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import os \n",
        "word2vec_model_path = '/content/drive/MyDrive/NLP/text-classfication/wordvectors-master/baomoi.vn.model.bin'\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_model_path,binary=True,unicode_errors='igore')\n",
        "vocab = model.wv.vocab\n",
        "wv= model.wv\n"
      ],
      "metadata": {
        "id": "Ke-XawBSv4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mã hoá \n",
        "def get_word2vec_data(X):\n",
        "    word2vec_data = []\n",
        "    for x in X:\n",
        "        sentence = []\n",
        "        for word in x.split(\" \"):\n",
        "            if word in vocab:\n",
        "                print(word)\n",
        "                sentence.append(wv[word])\n",
        "\n",
        "        word2vec_data.append(sentence)\n",
        "#         break\n",
        "    return get_word2vec_data\n",
        "\n",
        "X_data_w2v = get_word2vec_data(X_train)\n",
        "X_test_w2v = get_word2vec_data(X_test)"
      ],
      "metadata": {
        "id": "Aeww2cn0cDoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words"
      ],
      "metadata": {
        "id": "r7Od-hMaVFQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bow(data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  databow = tokenizer.texts_to_sequences(data)\n",
        "  databow= pad_sequences(databow, maxlen=300)\n",
        "  return databow"
      ],
      "metadata": {
        "id": "BTazspdtvulb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "databow_train = bow(X_train)\n",
        "databow_test = bow(X_test)\n",
        "svd = TruncatedSVD(n_components=300, random_state=42,)\n",
        "X_data_bow_dimen =svd.fit(x_train_tfidf).transform(x_train_tfidf)\n",
        "X_test_bow_dimen= svd.fit(x_test_tfidf).transform(x_test_tfidf)"
      ],
      "metadata": {
        "id": "UJCPySbLw_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CÁC PHƯƠNG PHÁP HỌC MÁY DÙNG ĐỂ PHÂN LOẠI ***"
      ],
      "metadata": {
        "id": "0glTMmo4hTeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert y to categorical"
      ],
      "metadata": {
        "id": "yZkIsprQWKj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train_n = encoder.fit_transform(y_train)\n",
        "y_test_n = encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "gF3enAT6WCaF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iM7TRbeWOVO",
        "outputId": "41722a6c-49e1-4686-eb6f-29ac1c8aacd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Chinh tri Xa hoi', 'Doi song', 'Kinh doanh'], dtype='<U16')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm train model "
      ],
      "metadata": {
        "id": "s6dncLK-W8Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sHgj4QeIXATN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=5000):       \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
        "    \n",
        "    if is_neuralnet:\n",
        "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
        "        \n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        val_predictions = val_predictions.argmax(axis=-1)\n",
        "        test_predictions = test_predictions.argmax(axis=-1)\n",
        "    else:\n",
        "        classifier.fit(X_train, y_train)\n",
        "    \n",
        "        train_predictions = classifier.predict(X_train)\n",
        "        val_predictions = classifier.predict(X_val)\n",
        "        test_predictions = classifier.predict(X_test)\n",
        "        \n",
        "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
        "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))\n",
        "    print(classification_report(test_predictions, y_test))"
      ],
      "metadata": {
        "id": "12dsSY4VXC9A"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "I9s0tKnGXaL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data_tfidf_ngram_dimen, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9V0bRRrsXcZr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf=svm.SVC(kernel='linear',C=100)\n",
        "clf.fit(X_train, y_train)\n",
        "val_predictions = clf.predict(X_test_tfidf_ngram_dimen)\n",
        "print(classification_report(val_predictions, y_test)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J35uEVKdZ_8R",
        "outputId": "4e316915-ab6d-4b13-dcfa-1aaabf1756fb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Chinh tri Xa hoi       0.68      0.63      0.65      8238\n",
            "        Doi song       0.86      0.50      0.63      3484\n",
            "      Kinh doanh       0.34      0.58      0.43      3159\n",
            "\n",
            "        accuracy                           0.59     14881\n",
            "       macro avg       0.63      0.57      0.57     14881\n",
            "    weighted avg       0.65      0.59      0.60     14881\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAIVE BAYES"
      ],
      "metadata": {
        "id": "bn1OonykaEm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(naive_bayes.BernoulliNB(),X_data_tfidf_ngram_dimen,y_train,X_test_tfidf_ngram_dimen,y_test,is_neuralnet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjyzB0ZU-lb_",
        "outputId": "b2a598dc-d162-4256-cecb-7f45ff3286fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy:  0.8392694063926941\n",
            "Test accuracy:  0.4164370674013843\n",
            "['Doi song' 'Chinh tri Xa hoi' 'Chinh tri Xa hoi' ... 'Chinh tri Xa hoi'\n",
            " 'Doi song' 'Kinh doanh']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sOMu0oxwXqeB",
        "2Y8vFk_9Yf0i",
        "Lv1XFZn9i0DQ",
        "AWDQEDn-ta3n",
        "G-bev__Ft9uS",
        "ZqVhihz_ymfH",
        "2OOF9fdS2YCp",
        "CtU7JPps3cJs"
      ],
      "mount_file_id": "1fi_hbNUPdfP9_Uh4sHsgieisJMcb8IYR",
      "authorship_tag": "ABX9TyOXsvwJTOQUVKgwjLig46d2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}